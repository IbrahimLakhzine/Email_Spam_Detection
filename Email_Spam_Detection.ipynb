{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Email_Spam_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Building a spam classifier using Naive Bayes"
      ],
      "metadata": {
        "id": "LAjVDJscM8i9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXDrPyLGpUs3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Importing the CountVectorizer to convert raw text to numbers\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#Importing the confusion matrix methods to check the performance of the model and visualise it.\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score \n",
        "import seaborn as sns\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ],
      "metadata": {
        "id": "JlhkCBewNJgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(path,sep,skiprows):\n",
        "    data=pd.read_csv(path,sep,names=['type', 'text'], \n",
        "                 skiprows=skiprows)\n",
        "    return data\n",
        "#Normalizing all datasets with the same header\n",
        "data1=loadData(\"TrainDataset1.csv\",\",\",1)\n",
        "data2=loadData(\"TrainDataset2.csv\",\",\",1)\n",
        "data3=loadData(\"TrainDataset3.txt\",\"\t\",0)\n",
        "print(\"\\t-------------TrainDataset1-------------\\n\")\n",
        "print(data1)\n",
        "print(\"\\t-------------TrainDataset2-------------\\n\")\n",
        "print(data2)\n",
        "print(\"\\t-------------TrainDataset3-------------\\n\")\n",
        "print(data3)"
      ],
      "metadata": {
        "id": "wIKA6jCTNOxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the data"
      ],
      "metadata": {
        "id": "mrSc0GsMNSQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcessing(dataTraining,data):\n",
        "    #extract text from dataset\n",
        "    texts=list(data[data.columns[-1]])\n",
        "    #make all words in lowerCase \n",
        "    texts=list(map(str.lower, texts))\n",
        "    \n",
        "    #remove punctuation\n",
        "    for i in range(len(texts)):\n",
        "        texts[i] = re.sub(r'[^\\w\\s]',' ',texts[i])\n",
        "        \n",
        "    #remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for i in range(len(texts)):\n",
        "        word_tokens = word_tokenize(texts[i])\n",
        "        myList = [w for w in word_tokens if not w in stop_words]\n",
        "        texts[i]=\" \".join(myList)\n",
        "            \n",
        "    #creat a new data with clean messages ( no punctuation no stopwords)\n",
        "    columns=[data.columns[0], data.columns[1]]\n",
        "    cleanData = pd.DataFrame(columns=columns)\n",
        "    cleanData[data.columns[0]] = list(data[data.columns[0]])\n",
        "    cleanData[data.columns[1]] = texts\n",
        "    \n",
        "    #Add the new data to the bigData for the training\n",
        "    dataTraining= pd.concat([dataTraining,cleanData],ignore_index=True)\n",
        "        \n",
        "    return dataTraining\n",
        "\n",
        "#creating and initializing the bigData for the training\n",
        "columns=['type', 'text']\n",
        "dataTraining = pd.DataFrame(columns=columns)\n",
        "dataTraining = dataTraining.fillna(0)\n",
        "\n",
        "#Combine the 3 dataset into one big dataset for the training.\n",
        "dataTraining= preProcessing(dataTraining,data1)\n",
        "dataTraining= preProcessing(dataTraining,data2)\n",
        "dataTraining= preProcessing(dataTraining,data3)\n",
        "dataTraining"
      ],
      "metadata": {
        "id": "w4JzLjb7NYok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualising the data"
      ],
      "metadata": {
        "id": "Dlgm43ovNlSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise the data\n",
        "\n",
        "#function that count the number of each ham word that exist within our bigData\n",
        "def count_words_ham(texts,types):\n",
        "    data={}\n",
        "\n",
        "    for i in range(len(texts)):\n",
        "        if types[i]==\"ham\":\n",
        "            text=texts[i] \n",
        "            listOfWords=text.split(\" \")\n",
        "            for k in range(len(listOfWords)):\n",
        "                if listOfWords[k] in data:\n",
        "                    data[listOfWords[k]]=data[listOfWords[k]]+1\n",
        "                else:\n",
        "                    data[listOfWords[k]]=1\n",
        "        else:\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "\n",
        "#function that count the number of each ham word that exist within our bigData\n",
        "def count_words_spam(texts,types):\n",
        "    data={}\n",
        "\n",
        "    for i in range(len(texts)):\n",
        "        if types[i]==\"spam\":\n",
        "            text=texts[i] \n",
        "            listOfWords=text.split(\" \")\n",
        "            for k in range(len(listOfWords)):\n",
        "                if listOfWords[k] in data:\n",
        "                    data[listOfWords[k]]=data[listOfWords[k]]+1\n",
        "                else:\n",
        "                    data[listOfWords[k]]=1\n",
        "        else:\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "data_ham=count_words_ham(dataTraining[dataTraining.columns[-1]].tolist(),dataTraining[dataTraining.columns[0]].tolist())                    \n",
        "data_spam=count_words_spam(dataTraining[dataTraining.columns[-1]].tolist(),dataTraining[dataTraining.columns[0]].tolist())\n",
        "\n",
        "\n",
        "\n",
        "#sort the dictionnary \n",
        "data_ham= sorted(data_ham.items(), key=lambda x: x[1], reverse=True)\n",
        "data_spam= sorted(data_spam.items(),key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "#we want to visualize the 8 most repeated words in spam and ham data\n",
        "dataToVisualize_ham={}\n",
        "dataToVisualize_spam={}\n",
        "\n",
        "#copy elements of a dictionary in another dict for ham\n",
        "def copy(numOfElement,data,dataToVisualize):\n",
        "    k=0\n",
        "    for i in data:\n",
        "        if k<numOfElement:\n",
        "            dataToVisualize[i[0]]=i[1]\n",
        "            k+=1 \n",
        "        else:break \n",
        "    \n",
        "    return dataToVisualize\n",
        "\n",
        "dataToVisualize_ham=copy(8,data_ham,dataToVisualize_ham)\n",
        "dataToVisualize_spam=copy(8,data_spam,dataToVisualize_spam)    \n",
        "    \n",
        "\n",
        "#exploring the dataset\n",
        "print(\"----exploring the dataset----\\n\")\n",
        "print(dataTraining['type'].value_counts())\n",
        "\n",
        "#plotting\n",
        "width = 0.30  # the width of the bars\n",
        "plt.bar(dataToVisualize_ham.keys(),dataToVisualize_ham.values(), width,color=\"r\")\n",
        "plt.title(\"ham\",color=\"r\")\n",
        "plt.show()\n",
        "\n",
        "plt.bar(dataToVisualize_spam.keys(),dataToVisualize_spam.values(), width,color=\"g\")\n",
        "plt.title(\"spam\",color=\"g\")\n",
        "plt.show()\n",
        "\n",
        "dataTraining[dataTraining.columns[0]].hist(bins=2)"
      ],
      "metadata": {
        "id": "HT3lOoNDNqgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building, training and validating the classifer"
      ],
      "metadata": {
        "id": "86WxLjDFN9ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build, train and validate the classifier,\n",
        "\n",
        "X=dataTraining[dataTraining.columns[-1]].tolist()\n",
        "Y= dataTraining[dataTraining.columns[0]].tolist()\n",
        "\n",
        "#split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "#initialize the vectorizer\n",
        "vectorizer=CountVectorizer()\n",
        "\n",
        "#initialize the model\n",
        "clf = MultinomialNB()\n",
        "\n",
        "#function to do the training,the prediction and evaluate the model\n",
        "def doAll(clf,vectorizer,step,x_train,x_test,y_train,y_test):\n",
        "    if(step==\"step4\"):\n",
        "        x_train = vectorizer.fit_transform(x_train)\n",
        "        x_test = vectorizer.transform(x_test)\n",
        "        \"\"\"\n",
        "           Making an object of the MultinomialNB class followed by \n",
        "           fitting the classifier object on X_train and y_train data.\n",
        "            .toarray() with X_train is used to \n",
        "           convert a sparse matrix to a dense matrix.\n",
        "        \"\"\"   \n",
        "        \n",
        "        clf.fit(x_train.toarray(), y_train)\n",
        "        #Predicting the test set results\n",
        "        y_pred = clf.predict(x_test.toarray())\n",
        "\n",
        "\n",
        "        \"\"\" Checking the performance of the model \"\"\"\n",
        "        #Making the Confusion Matrix and classification_report\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        cr=classification_report(y_test, y_pred)\n",
        "        ac=accuracy_score(y_test, y_pred)\n",
        "\n",
        "        return cr,cm,ac\n",
        "       \n",
        "    \n",
        "    elif(step==\"step5\"):\n",
        "        #vectorize the data\n",
        "        x_test = vectorizer.transform(x_test)\n",
        "        #prediction\n",
        "        y_pred_SMS = clf.predict(x_test.toarray())\n",
        "        \n",
        "        \"\"\" Checking the performance of the model \"\"\"\n",
        "        #Making the Confusion Matrix\n",
        "        cmSMS = confusion_matrix(y_test_SMS,y_pred_SMS)\n",
        "        crSMS=classification_report(y_test_SMS, y_pred_SMS)\n",
        "        acSMS=accuracy_score(y_test_SMS, y_pred_SMS)\n",
        "        \n",
        "        return crSMS,cmSMS,acSMS\n",
        "\n",
        "cr,cm,ac=doAll(clf,vectorizer,\"step4\",x_train,x_test,y_train,y_test)        \n",
        "\n",
        "\"\"\" Visualisation \"\"\"\n",
        "def visualFunction(ac,cr,cm,color):\n",
        "    print (\"\\t\\t----------Accuracy Score ----------\\nAccuracy=%.1f\"% (ac*100),\"%\")\n",
        "\n",
        "    print(\"\\n\\t\\t----------Report----------\\n\")\n",
        "    print(cr)\n",
        "    #Visualising the Confusion Matrix\n",
        "    print(\"\\n\\t\\t------Confusion Matrix------\\n\")\n",
        "    print(cm)\n",
        "\n",
        "    #heatmap for confusion matrix\n",
        "    print(\"\\n\\t\\t------heatmap for confusion matrix------\\n\")\n",
        "\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in  zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    sns.heatmap(cm, annot=labels, fmt='', cmap=color)\n",
        "    return\n",
        "\n",
        "visualFunction(ac,cr,cm,'Reds')"
      ],
      "metadata": {
        "id": "wBqrLNCaOEma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "supervised classification"
      ],
      "metadata": {
        "id": "IhirHmLDOeLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the classifier\n",
        "\n",
        "#load the data\n",
        "data4=loadData(\"SMSSpamCollection.txt\",\"\t\",0)\n",
        "columns=['type', 'text']\n",
        "SMSData = pd.DataFrame(data4.values.tolist(),columns=columns)\n",
        "y_test_SMS=SMSData[SMSData.columns[0]].tolist()\n",
        "x_test_SMS=SMSData[SMSData.columns[-1]].tolist()\n",
        "\n",
        "crSMS,cmSMS,acSMS=doAll(clf,vectorizer,\"step5\",None,x_test_SMS,None,y_test_SMS)\n",
        "\n",
        "\"\"\"visualization\"\"\"\n",
        "visualFunction(acSMS,crSMS,cmSMS,'terrain')"
      ],
      "metadata": {
        "id": "L0_gQsG_ONfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised classification"
      ],
      "metadata": {
        "id": "eQ5kj2s7OTZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the classifier\n",
        "def unsupPred(x_dataTest):\n",
        "    columns=['type','text']\n",
        "    unsupPredData = pd.DataFrame(columns=columns)\n",
        "    unsupPredData[unsupPredData.columns[1]] = x_dataTest\n",
        "    #vectorize the data\n",
        "    x_dataTest = vectorizer.transform(x_dataTest)\n",
        "    #prediction\n",
        "    y_pred_dataTest = clf.predict(x_dataTest.toarray())\n",
        "\n",
        "    #convert list to dataframe\n",
        "    \n",
        "    unsupPredData[unsupPredData.columns[0]] = y_pred_dataTest\n",
        "    \n",
        "    return unsupPredData\n",
        "#load data\n",
        "data5=pd.read_csv(\"TestDataset.csv\",names=['type'],skiprows=1)\n",
        "x_unspData=data5[data5.columns[0]]\n",
        "\n",
        "unsupPredData=unsupPred(x_unspData)\n",
        "print(\"\\n\\t------TestDataset with predicted labels------\\n\")\n",
        "print(unsupPredData)\n",
        "\n",
        "#exploring the dataset\n",
        "print(\"\\n\\n\\t----exploring the predTestDataset----\\n\")\n",
        "print(unsupPredData['type'].value_counts())\n",
        "\n",
        "unsupPredData[unsupPredData.columns[0]].hist(bins=2)"
      ],
      "metadata": {
        "id": "uP6rd2PSOSl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cheating the classifier"
      ],
      "metadata": {
        "id": "SlkaLcJJOk1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cheat the classifier\n",
        "data6=pd.read_csv(\"TestDataset.csv\",names=['type'],skiprows=1)\n",
        "newRows=[[\"Since our debut a year ago more than a million people have joined our community\"],\n",
        "         [\"Oh! no share Market has fallen down by $100,000 due to Corona outbreak... \"],\n",
        "         [\"ok, got ur call\"],\n",
        "         [\"get a free call\"],\n",
        "         [\"hi, u get text 4 free\"]]\n",
        "\n",
        "data6 = data6.append(pd.DataFrame(newRows, columns=['type']),ignore_index=True)\n",
        "\n",
        "newRows=data6[data6.columns[0]]\n",
        "newRows = vectorizer.transform(newRows)\n",
        "y_pred_new = clf.predict(newRows.toarray())\n",
        "print(\"\\n\\t----Predection of the new 5 sentences----\\n\")\n",
        "print(y_pred_new[-5:])\n",
        "\n",
        "#exploring the dataset\n",
        "columns=['type']\n",
        "cheatPredData = pd.DataFrame(y_pred_new,columns=columns)\n",
        "print(\"\\n\\t----exploring the cheatPredDataset----\\n\")\n",
        "print(cheatPredData['type'].value_counts())\n",
        "cheatPredData[cheatPredData.columns[0]].hist(bins=2)"
      ],
      "metadata": {
        "id": "68Ip-qAhOmRI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}